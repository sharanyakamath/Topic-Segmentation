{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1PqNa6v2osz",
        "outputId": "3c352ef0-bd76-48b2-9966-4471a0b57c69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFc4VJGnU10X"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HTJB7Ee75FnJ"
      },
      "outputs": [],
      "source": [
        "!python3 /content/drive/MyDrive/SEM2/696DS/cross-segment/config_setup.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsCR7nEKVtE-",
        "outputId": "298fa65b-028f-4c34-909d-2f95563efec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pathlib2\n",
            "  Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pathlib2) (1.15.0)\n",
            "Installing collected packages: pathlib2\n",
            "Successfully installed pathlib2-2.3.7.post1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pathlib2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGAU0SV8WP-R",
        "outputId": "1de0b6b6-25fa-4593-ecca-d89f11ac07c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 38.6 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 32.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 30.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9K9IqhYCyb6f"
      },
      "outputs": [],
      "source": [
        "!cd /content/drive/MyDrive/SEM2/696DS/cross-segment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsJbzAnEj6Y1"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "transformers.logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYfF8abzsEK9",
        "outputId": "c633f730-be8e-46ae-89aa-0f52a53c712c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/content/drive/MyDrive/SEM2/696DS/cross-segment/dataset/data_out/data_out\n",
            "/content/drive/MyDrive/SEM2/696DS/cross-segment/dataset/data_out/data_out/paths_cache.txt\n",
            "loading names of all files\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/SEM2/696DS/cross-segment/training.py\", line 171, in <module>\n",
            "    dataset_train = WikipediaDataSet(dataset_path+'data_out',n_context_sent= n_context_sent, high_granularity=False)\n",
            "  File \"/content/drive/MyDrive/SEM2/696DS/cross-segment/wiki_loader.py\", line 151, in __init__\n",
            "    cache_wiki_filenames(root_path)\n",
            "  File \"/content/drive/MyDrive/SEM2/696DS/cross-segment/wiki_loader.py\", line 69, in cache_wiki_filenames\n",
            "    with open(cache_file_path,'w') as f:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/SEM2/696DS/cross-segment/dataset/data_out/data_out/paths_cache.txt'\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/drive/MyDrive/SEM2/696DS/cross-segment/training.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO4IzQaY2lLW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "#from text_manipulation import word_model\n",
        "#from text_manipulation import extract_sentence_words\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from pathlib2 import Path\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import gensim\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmQmyTTPcJQw",
        "outputId": "6ea58d39-adfb-48e5-d13c-585c4b116487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXEQxtdL2lLb",
        "outputId": "38052d84-6031-4125-a8f1-6986e0c00210"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import AdamW\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "MAX_TOKENS = 200\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dunv5ClS2lLc"
      },
      "outputs": [],
      "source": [
        "#logger = utils.setup_logger(__name__, 'train.log')\n",
        "missing_stop_words = set(['of', 'a', 'and', 'to'])\n",
        "section_delimiter = \"========\"\n",
        "segment_seperator = \"========\"\n",
        "\n",
        "def get_list_token():\n",
        "    return \"***LIST***\"\n",
        "\n",
        "def get_formula_token():\n",
        "    return \"***formula***\"\n",
        "\n",
        "def get_codesnipet_token():\n",
        "    return \"***codice***\"\n",
        "\n",
        "def get_special_tokens():\n",
        "    special_tokens = []\n",
        "    special_tokens.append(get_list_token())\n",
        "    special_tokens.append(get_formula_token())\n",
        "    special_tokens.append(get_codesnipet_token())\n",
        "    return special_tokens\n",
        "\n",
        "def get_seperator_foramt(levels = None):\n",
        "    level_format = '\\d' if levels == None else '['+ str(levels[0]) + '-' + str(levels[1]) + ']'\n",
        "    seperator_fromat = segment_seperator + ',' + level_format + \",.*?\\.\"\n",
        "    return seperator_fromat\n",
        "\n",
        "words_tokenizer = None\n",
        "def get_words_tokenizer():\n",
        "    global words_tokenizer\n",
        "\n",
        "    if words_tokenizer:\n",
        "        return words_tokenizer\n",
        "\n",
        "    words_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    return words_tokenizer\n",
        "\n",
        "def extract_sentence_words(sentence, remove_missing_emb_words = False, remove_special_tokens = False):\n",
        "    if (remove_special_tokens):\n",
        "        for token in get_special_tokens():\n",
        "            # Can't do on sentence words because tokenizer delete '***' of tokens.\n",
        "            sentence = sentence.replace(token, \"\")\n",
        "    tokenizer = get_words_tokenizer()\n",
        "    sentence_words = tokenizer.tokenize(sentence)\n",
        "    if remove_missing_emb_words:\n",
        "        sentence_words = [w for w in sentence_words if w not in missing_stop_words]\n",
        "\n",
        "    return sentence_words\n",
        "\n",
        "\n",
        "def word_model(word, model): #http://127.0.0.1:8990/?token=bf09ebab079e0fa44f92e9781373743116328722bca9e067\n",
        "    if model is None:\n",
        "        return np.random.randn(1, 300)\n",
        "    else:\n",
        "        if word in model:\n",
        "            return model[word].reshape(1, 300)\n",
        "        else:\n",
        "            #print ('Word missing w2v: ' + word)\n",
        "            return model['UNK'].reshape(1, 300)\n",
        "\n",
        "def get_files(path):\n",
        "    all_objects = Path(path).glob('**/*')\n",
        "    files = [str(p) for p in all_objects if p.is_file()]\n",
        "    return files\n",
        "\n",
        "\n",
        "def get_cache_path(wiki_folder):\n",
        "    cache_file_path = wiki_folder / 'paths_cache'\n",
        "    return cache_file_path\n",
        "\n",
        "\n",
        "def cache_wiki_filenames(wiki_folder):\n",
        "    files = Path(wiki_folder).glob('*/*/*/*')\n",
        "    cache_file_path = get_cache_path(wiki_folder)\n",
        "\n",
        "    with cache_file_path.open('w+') as f:\n",
        "        for file in files:\n",
        "            f.write(file + u'\\n')\n",
        "\n",
        "\n",
        "def clean_section(section):\n",
        "    cleaned_section = section.strip('\\n')\n",
        "    return cleaned_section\n",
        "\n",
        "\n",
        "def get_scections_from_text(txt, high_granularity=True):\n",
        "    sections_to_keep_pattern = get_seperator_foramt() if high_granularity else get_seperator_foramt(\n",
        "        (1, 2))\n",
        "    if not high_granularity:\n",
        "        # if low granularity required we should flatten segments within segemnt level 2\n",
        "        pattern_to_ommit = get_seperator_foramt((3, 999))\n",
        "        txt = re.sub(pattern_to_ommit, \"\", txt)\n",
        "\n",
        "        #delete empty lines after re.sub()\n",
        "        sentences = [s for s in txt.strip().split(\"\\n\") if len(s) > 0 and s != \"\\n\"]\n",
        "        txt = '\\n'.join(sentences).strip('\\n')\n",
        "\n",
        "\n",
        "    all_sections = re.split(sections_to_keep_pattern, txt)\n",
        "    non_empty_sections = [s for s in all_sections if len(s) > 0]\n",
        "\n",
        "    return non_empty_sections\n",
        "\n",
        "\n",
        "def get_sections(path, high_granularity=True):\n",
        "    file = open(str(path), \"r\")\n",
        "    raw_content = file.read()\n",
        "    file.close()\n",
        "\n",
        "    clean_txt = raw_content.strip()\n",
        "\n",
        "    sections = [clean_section(s) for s in get_scections_from_text(clean_txt, high_granularity)]\n",
        "\n",
        "    return sections\n",
        "\n",
        "def read_wiki_file(path, n_context_sent = 1, remove_preface_segment=True, high_granularity=True):\n",
        "    data = []\n",
        "    targets = []\n",
        "    all_sections = get_sections(path, high_granularity)\n",
        "    required_sections = all_sections[1:] if remove_preface_segment and len(all_sections) > 0 else all_sections\n",
        "    required_non_empty_sections = [section for section in required_sections if len(section) > 0 and section != \"\\n\"]\n",
        "\n",
        "    list_sentence = get_list_token() + \".\"\n",
        "    final_sentences = []\n",
        "    label = []\n",
        "    for section_ind in range(len(required_non_empty_sections)):\n",
        "        sentences_ = required_non_empty_sections[section_ind].split('\\n')\n",
        "        sentences = [x for x in sentences_ if x != list_sentence]\n",
        "        if sentences:\n",
        "            for sentence in sentences[:-1]:\n",
        "                final_sentences.append(sentence)\n",
        "                label.append(0)\n",
        "            final_sentences.append(sentences[-1])\n",
        "            label.append(1)\n",
        "    \n",
        "    if len(final_sentences)>n_context_sent:\n",
        "        for sent_ind in range(n_context_sent,len(final_sentences)):\n",
        "            prev_context = final_sentences[sent_ind-n_context_sent:sent_ind]\n",
        "            after_context = final_sentences[sent_ind: min(len(final_sentences),sent_ind+n_context_sent)]\n",
        "            \n",
        "            prev_context = \" \".join(prev_context)\n",
        "            after_context = \" \".join(after_context)\n",
        "            data.append([prev_context, after_context])\n",
        "            targets.append(label[sent_ind-1])\n",
        "\n",
        "    return data, targets, path\n",
        "\n",
        "\n",
        "class WikipediaDataSet(Dataset):\n",
        "    def __init__(self, root,n_context_sent = 1, train=True, manifesto=False, folder=False, high_granularity=False):\n",
        "\n",
        "        if (manifesto):\n",
        "            self.textfiles = list(Path(root).glob('*'))\n",
        "        else:\n",
        "            if (folder):\n",
        "                self.textfiles = get_files(root)\n",
        "            else:\n",
        "                root_path = Path(root)\n",
        "                print(root_path)\n",
        "                cache_path = get_cache_path(root_path)\n",
        "                print(cache_path)\n",
        "                if not cache_path.exists():\n",
        "                    print(\"not_exist\")\n",
        "                    cache_wiki_filenames(root_path)\n",
        "                self.textfiles = cache_path.read_text().splitlines()\n",
        "\n",
        "        if len(self.textfiles) == 0:\n",
        "            raise RuntimeError('Found 0 images in subfolders of: {}'.format(root))\n",
        "        self.train = train\n",
        "        self.root = root\n",
        "        self.high_granularity = high_granularity\n",
        "        self.n_context_sent = n_context_sent\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.textfiles[index]\n",
        "\n",
        "        return read_wiki_file(Path(path),n_context_sent = 2,high_granularity=self.high_granularity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.textfiles)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "r36Xr0Xs2lLj",
        "outputId": "8a79dfab-423e-4f41-af9c-717d58bae8b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/SEM2/696DS/supervised-learning/wiki_727/dev\n",
            "/content/drive/MyDrive/SEM2/696DS/supervised-learning/wiki_727/dev/paths_cache\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2bdf31f54686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/SEM2/696DS/supervised-learning/wiki_727\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWikipediaDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/dev'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh_granularity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ce469543e745>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, n_context_sent, train, manifesto, folder, high_granularity)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextfiles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found 0 images in subfolders of: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found 0 images in subfolders of: /content/drive/MyDrive/SEM2/696DS/supervised-learning/wiki_727/dev"
          ]
        }
      ],
      "source": [
        "\n",
        "def collate_fn(batch):\n",
        "    batched_data = []\n",
        "    batched_targets = []\n",
        "    batched_paths = []\n",
        "\n",
        "    window_size = 1\n",
        "    before_sentence_count = int(math.ceil(float(window_size - 1) /2))\n",
        "    after_sentence_count = window_size - before_sentence_count - 1\n",
        "    max_tokens = 100\n",
        "    for data, targets, path in batch:\n",
        "        try:\n",
        "            for i in range(len(data)):\n",
        "                temp = len(data[i][0].split())+len(data[i][1].split())\n",
        "                if max_tokens < temp:\n",
        "                    max_tokens = temp\n",
        "                batched_data.append(data[i])\n",
        "                batched_targets.append(targets[i])\n",
        "                batched_paths.append(path)\n",
        "        except Exception as e:\n",
        "            logger.info('Exception \"%s\" in file: \"%s\"', e, path)\n",
        "            logger.debug('Exception!', exc_info=True)\n",
        "            continue\n",
        "    \n",
        "    max_tokens = min(MAX_TOKENS, max_tokens)\n",
        "    tokens = tokenizer(\n",
        "                    batched_data,\n",
        "                    padding = True,\n",
        "                    max_length = max_tokens,\n",
        "                    truncation=True)\n",
        "    '''seq = torch.tensor(tokens['input_ids'])\n",
        "    mask = torch.tensor(tokens['attention_mask'])\n",
        "    y = torch.tensor(batched_targets)'''\n",
        "        \n",
        "    return tokens['input_ids'], tokens['attention_mask'], batched_targets, batched_paths\n",
        "\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/SEM2/696DS/supervised-learning/wiki_727\"\n",
        "dataset = WikipediaDataSet(dataset_path+'/dev', high_granularity=False)\n",
        "dl = DataLoader(dataset_path, batch_size=12, collate_fn = collate_fn, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tG5pa0I2lLk",
        "outputId": "82f8dc2b-1036-4df5-bee7-190d6e6aba78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "print(len(dl))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "Bpn2zVzZ2lLk",
        "outputId": "0ecd8683-9b2c-40e7-adaa-407f009d7015"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-dc6e303a8178>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute '__getitem__'"
          ]
        }
      ],
      "source": [
        "sent,target,path = dataset.__getitem__(1)\n",
        "print(sent[0][0])\n",
        "print(len(sent),len(target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG7QLPG42lLl"
      },
      "outputs": [],
      "source": [
        "print(len(sent[0]))\n",
        "print(sent[1])\n",
        "print(sent[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyZjAnR22lLl"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer(\n",
        "                    sent,\n",
        "                    padding = True,\n",
        "                    max_length = 200,\n",
        "                    truncation=True)\n",
        "print(tokenizer.decode(tokens['input_ids'][2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDbYDZep2lLm"
      },
      "outputs": [],
      "source": [
        "print(len(dl))\n",
        "_,batch = next(enumerate(dl))\n",
        "print(len(batch))\n",
        "input_, mask, targets, paths = batch\n",
        "print(len(input_),len(mask),len(targets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGnZAKJT2lLn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoSERvPc2lLn"
      },
      "outputs": [],
      "source": [
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCTi3YdY2lLn"
      },
      "outputs": [],
      "source": [
        "class Encoder_Classifier(nn.Module):\n",
        "    def __init__(self, bert, n_classes):\n",
        "        super(Encoder_Classifier, self).__init__()\n",
        "        self.bert = bert\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        # relu activation function\n",
        "        self.relu =  nn.ReLU()\n",
        "\n",
        "        # dense layer 1\n",
        "        self.fc1 = nn.Linear(768,n_classes)\n",
        "        \n",
        "        #softmax activation function\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "        #pass the inputs to the model  \n",
        "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "\n",
        "\n",
        "        x = self.fc1(cls_hs) \n",
        "\n",
        "        # apply softmax activation\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZQkhCy_2lLo"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader):\n",
        "    model.train()\n",
        "\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    '''\n",
        "    # empty list to save model predictions\n",
        "    total_preds=[]'''\n",
        "  \n",
        "    # iterate over batches\n",
        "    for step,batch in enumerate(train_dataloader):\n",
        "\n",
        "        # push the batch to gpu\n",
        "        #batch = [r.to(device) for r in batch]\n",
        "\n",
        "        ###### for  labeled data, computing cross entropy   #########\n",
        "        sent_id, mask, labels = torch.tensor(batch[0]).to(device),torch.tensor(batch[1]).to(device),torch.tensor(batch[2]).to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "        preds = model(sent_id, mask)\n",
        "        loss = CELoss(preds, labels)\n",
        "\n",
        "        # backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        #torch.cuda.empty_cache()\n",
        "        # add on to the total loss\n",
        "        loss_item = loss.item()\n",
        "        total_loss += loss_item\n",
        "\n",
        "        # progress update after every 100 batches.\n",
        "        if step % 100 == 0 and not step == 0:\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "            print(\"loss\",loss_item)\n",
        "\n",
        "         \n",
        "\n",
        "        '''\n",
        "        # model predictions are stored on GPU. So, push it to CPU\n",
        "        preds=preds.detach().cpu().numpy()\n",
        "\n",
        "        # append the model predictions\n",
        "        total_preds.append(preds)'''\n",
        "\n",
        "    # compute the training loss of the epoch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    #total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "    #returns the loss and predictions\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elnuJJNJ2lLp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "def evaluate(dev_dataloader):\n",
        "  \n",
        "    print(\"\\nEvaluating...\")\n",
        "  \n",
        "    # deactivate dropout layers\n",
        "    model.eval()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    total_preds = [[None,None]]\n",
        "    count=0\n",
        "    curr_examples = 0\n",
        "\n",
        "    # iterate over batches\n",
        "    for step,batch in enumerate(dev_dataloader):\n",
        "    \n",
        "        # Progress update every 50 batches.\n",
        "        if step % 1 == 0 and not step == 0:\n",
        "      \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,} accuracy {}.'.format(step, len(dev_dataloader), count/curr_examples))\n",
        "            temp = np.delete(total_preds,0,0)\n",
        "            print(\"F1 score {}\".format(f1_score(list(temp[:,0]),list(temp[:,1]),average=\"macro\")))\n",
        "        # push the batch to gpu\n",
        "        #batch = [t.to(device) for t in batch]\n",
        "    \n",
        "\n",
        "        sent_id, mask, labels = torch.tensor(batch[0]).to(device),torch.tensor(batch[1]).to(device),torch.tensor(batch[2]).to(device)\n",
        "        curr_examples += len(sent_id)\n",
        "        # deactivate autograd\n",
        "        with torch.no_grad():\n",
        "      \n",
        "        # model predictions\n",
        "            preds = model(sent_id, mask)\n",
        "\n",
        "            # compute the validation loss between actual and predicted values\n",
        "            loss = CELoss(preds,labels)\n",
        "\n",
        "            total_loss = total_loss + loss.item()\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "\n",
        "            true_class = np.expand_dims(np.argmax(preds,axis=1),axis=0)\n",
        "            labels = np.expand_dims(labels.detach().cpu().numpy(), axis=0)\n",
        "            temp = np.concatenate((labels,true_class),axis=0).T\n",
        "            \n",
        "            total_preds = np.concatenate((total_preds,temp),axis=0)\n",
        "\n",
        "            \n",
        "            for myvar in range(len(labels[0])):\n",
        "                if labels[0][myvar]== true_class[0][myvar]:\n",
        "                    count+=1\n",
        "\n",
        "    # compute the validation loss of the epoch\n",
        "    avg_loss = total_loss / len(dev_dataloader) \n",
        "\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    total_preds  = np.concatenate(total_preds, axis=0)\n",
        "    print(\"Validation accuracy\",count/len(dev_data))\n",
        "\n",
        "    return avg_loss, count/len(dev_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvMYBBnI2lLp"
      },
      "outputs": [],
      "source": [
        "best_valid_loss = float('inf')\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "accuracy_list = []\n",
        "learning_rate = .0005\n",
        "freezed_epochs = 1\n",
        "unfreezed_epochs = 1\n",
        "n_classes = 2\n",
        "n_layer_unfreeze = 2\n",
        "weight = .3\n",
        "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
        "CELoss = nn.CrossEntropyLoss()\n",
        "model = Encoder_Classifier(bert, n_classes).to(device)\n",
        "optimizer = AdamW(model.parameters(),lr = learning_rate)\n",
        "print(device)\n",
        "\n",
        "dataset_path = \"/home/aakash/amagi/data/wiki/wiki_727\"\n",
        "dataset_train = WikipediaDataSet(dataset_path+'/dev', high_granularity=False)\n",
        "train_dataloader = DataLoader(dataset_train, batch_size=2, collate_fn = collate_fn, shuffle=True)\n",
        "\n",
        "dataset_dev = WikipediaDataSet(dataset_path+'/test', high_granularity=False)\n",
        "dev_dataloader = DataLoader(dataset_dev, batch_size=2, collate_fn = collate_fn, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mry8O4CJ2lLq"
      },
      "outputs": [],
      "source": [
        "for epoch in range(freezed_epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, freezed_epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss = train(train_dataloader)\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, accuracy = evaluate(dev_dataloader)\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    accuracy_list.append(accuracy)\n",
        "\n",
        "    print(f'\\nTraining Loss: {train_loss[0]:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH30DYn_2lLq"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/home/aakash/amagi/data/wiki/wiki_727/saved_weights.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNY3XV_N2lLq"
      },
      "outputs": [],
      "source": [
        "##### unfreezing layers #######\n",
        "for iter in range(n_layer_unfreeze):\n",
        "    print(str(iter+1)+\" unfreeze\")\n",
        "    for param in model.bert.encoder.layer._modules[str(11-iter)].parameters():\n",
        "        param.requires_grad=True\n",
        "    for epoch in range(unfreezed_epochs):\n",
        "     \n",
        "        print('\\n Epoch {:} / {:}'.format(epoch+1 , ))\n",
        "        \n",
        "        #train model\n",
        "        train_loss = train(train_dataloader)\n",
        "    \n",
        "        #evaluate model\n",
        "        valid_loss, accuracy = evaluate(dev_dataloader)\n",
        "        \n",
        "        #save the best model\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "        \n",
        "        # append training and validation loss\n",
        "        train_losses.append(train_loss)\n",
        "        valid_losses.append(valid_loss)\n",
        "        accuracy_list.append(accuracy)\n",
        "        \n",
        "        print(f'\\nTraining Loss: {train_loss[0]:.3f}')\n",
        "        print(f'Validation Loss: {valid_loss:.3f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "cross_segment_bert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}